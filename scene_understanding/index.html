<!DOCTYPE html>
<html>
<title>Scene Understanding and Representation Learning</title>

<head>
  <meta charset="UTF-8" />
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="../css/element-ui.css" />
  <link rel="stylesheet" href="../css/index.css" />
  <script src="../js/vue.min.js"></script>
  <script src="../js/element-ui.min.js"></script>
</head>

<body>
  <div id="app">
    <el-page-header @back="goBack" title="Back"> </el-page-header>
    <!-- Scene Understanding and Representation Learning -->
    <div>
      <h1>Scene Understanding and Representation Learning</h1>
      <el-divider></el-divider>
      <div v-for="project,index in projects">
        <p class="project_p">
          <span v-html="textBold(project.authors)"></span>
          <b class="project_title" :id="index">{{project.title}}</b>
          <div id="index"></div>
          <span v-for="tag in project.tags" :style="styles[tag[1]]">{{tag[0]}}</span>
          <span v-for="link in project.links">[<b><a class="project_links"
                :href="link.link">{{link.name}}</a></b><span>]&nbsp;</span></span>
        </p>
        <div v-if="project.video" class="figure_div">
          <video width="640" :src="'videos/'+project.video" controls></video>
        </div>
        <div v-for="img in project.imgs" class="figure_div">
          <img :src="'figures/'+img.src" :alt="img" class="figure_img" />
          <div v-if="img.label">
            <div class="figure_label">{{img.label}}</div>
            <br>
          </div>
        </div>
        <div v-if="project.io">
          <p><b>Input-Output:&nbsp;</b>{{project.io}}<span></span></p>
        </div>
        <div v-if="project.abstract">
          <p><b>Abstract.&nbsp;</b>{{project.abstract}}<span></span></p>
        </div>
        <el-divider content-position="right">{{index+1}}</el-divider>
      </div>
    </div>
  </div>
</body>
<script src="../js/index.js"></script>
<script>
  new Vue({
    el: '#app',
    data: function () {
      return {
        styles,
        projects: [
          {
            authors: 'Yu Qiao*, Jincheng Zhu*, Chengjiang Long, Zeyao Zhang, Yuxin Wang, Zhenjun Du, Xin Yangâ€ .',
            title: 'CPRAL: Collaborative Panoptic-Regional Active Learning for Semantic Segmentation.',
            tags: [
              ['AAAI 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            video: 'CPRAL.mp4',
            io: 'Given an input semantic segmentation dataset, our network outputs a small subset that can represent the entire dataset. Then the model trained on the subset can match the performance trained on the full-annotated dataset.',
            abstract: 'Acquiring the most representative examples via active learning (AL) can benefit many data-dependent computer vision tasks by minimizing efforts of image-level or pixel-wise annotations. In this paper, we propose a novel Collaborative Panoptic-Regional Active Learning framework (CPRAL) to address the semantic segmentation task. For a small batch of images initially sampled with pixel-wise annotations, we employ panoptic information to initially select unlabeled samples. Considering the class imbalance in the segmentation dataset, we import a Regional Gaussian Attention module (RGA) to achieve semantics-biased selection. The subset is highlighted by vote entropy and then attended by Gaussian kernels to maximize the biased regions. We also propose a Contextual Labels Extension (CLE) to boost regional annotations with contextual attention guidance. With the collaboration of semantics-agnostic panoptic matching and regionbiased selection and extension, our CPRAL can strike a balance between labeling efforts and performance and compromise the semantics distribution. We perform extensive experiments on Cityscapes and BDD10K and show that CPRAL outperforms the cutting-edge methods with impressive results and less labeling proportion.',
            imgs: [
              { src: 'aaai_2022.png', label: 'The pipeline of the proposed active learning framework. The purple, green and gray stages correspond to the panoptic selection, regional selection and annotation phase. There are four phases in an iteration to finish the samples selection and annotation process. (1): Take unlabeled examples as input and extract multi-scale features to regress a matching rating. (2) Regional Gaussian Attention (RGA): Perform vote entropy, kernel filter, and non-maximum suppression (NMS) on panoptic samples to decide semantics-biased regions. (3) Oracle or our designed label tool can annotate the selected subset with high accuracy and then move them to the labeled pool. (4) Contextual Labels Extension (CLE): Take images, regional labels and masks as input, extract patches to boost annotations with contextual attention as guidance.' }
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/abs/2112.05975v1' }
            ]
          },
          {
            authors: 'Yidan Feng, Sen Deng, Xuefeng Yan, Xin Yang, Mingqiang Wei, and Ligang Liu.',
            title: 'Easy2Hard: Learning to Solve the Intractables from a Synthetic Dataset for Structure-preserving Image Smoothing.',
            tags: [
              ['IEEE Transactions on Neural Networks and Learning Systems 2021', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            abstract: 'Image smoothing is a prerequisite for many computer vision and graphics applications. In this paper, we raise an intriguing question whether a dataset that semantically describes meaningful structures and unimportant details, can facilitate a deep learning model to smooth complex natural images. To answer it, we generate ground-truth labels from easy samples by candidate generation and a screening test, and synthesize hard samples in structure-preserving smoothing by blending intricate and multifarious details with the labels. To take full advantage of this dataset, we present a joint edge detection and structure-preserving image smoothing neural network, which we call JESS-Net for short. Moreover, we propose the distinctive total variation loss as a prior knowledge to narrow the gap between synthetic and real data. Experiments on different datasets and real images show clear improvements of our method over the state-of-the-arts in terms of both the image cleanness and structure-preserving ability.',
            imgs: [
              { src: 'Easy2Hard1.png' },
              { src: 'Easy2Hard2.png' },
              { src: 'Easy2Hard3.png' },
              { src: 'Easy2Hard4.png' }
            ],
          },
          {
            authors: 'Ke Xu, Xin Tian, Xin Yang, Baocai Yin, Rynson Lau.',
            title: 'Intensity-Aware Single-Image Deraining with Semantic and Color Regularization.',
            tags: [
              ['IEEE Transactions on Image Processing 2021', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            abstract: 'Rain degrades image visual quality and disrupts object structures, obscuring their details and erasing their colors. Existing deraining methods are primarily based on modeling either visual appearances of rain or its physical characteristics (e.g., rain direction and density), and thus suffer from two common problems. First, due to the stochastic nature of rain, they tend to fail in recognizing rain streaks correctly, and wrongly remove image structures and details. Second, they fail to recover the image colors erased by heavy rain. In this paper, we address these two problems with the following three contributions. First, we propose a novel PHP block to aggregate comprehensive spatial and hierarchical information for removing rain streaks of different sizes. Second, we propose a novel network to first remove rain streaks, then recover objects structures/colors, and finally enhance details. Third, to train the network, we prepare a new dataset, and propose a novel loss function to introduce semantic and color regularization for deraining. Extensive experiments demonstrate the superiority of the proposed method over state-of-the-art deraining methods on both synthesized and real-world data, in terms of visual quality, quantitative accuracy, and running speed.',
            imgs: [
              { src: 'Intensity-Aware1.png' },
              { src: 'Intensity-Aware2.png' },
              { src: 'Intensity-Aware3.png' }
            ],
          },
          {
            authors:
              'Xin Yang, Zongliang Ma, Letian Yu, Ying Cao, Baocai Yin, Xiaopeng Wei, Qiang Zhang and Rynson Lau.',
            title: 'Automatic Comic Generation with Stylistic Multi-page Layouts and Emotion-driven Text Balloon Generation.',
            tags: [
              ['ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 2020', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            imgs: [
              { src: 'Automatic Comic1.jpg' },
              { src: 'Automatic Comic2.gif' }
            ],
            links: [
              {
                name: 'paper',
                link: 'https://arxiv.org/abs/2101.11111',
              },
              {
                name: 'video',
                link:
                  'figures/9327261e3f0ae6209d1302a5a1afa96a.mp4',
              },
              {
                name: 'media',
                link:
                  'https://mp.weixin.qq.com/s/z_QorcyPDOfw0PDYX-UgTg',
              },
            ],
            abstract: "In this paper, we propose a fully automatic system for generating comic books from videos without any human intervention. Given an input video along with its subtitles, our approach first extracts informative keyframes by analyzing the subtitles, and stylizes keyframes into comic-style images. Then, we propose a novel automatic multi-page layout framework, which can allocate the images across multiple pages and synthesize visually interesting layouts based on the rich semantics of the images (e.g., importance and inter-image relation). Finally, as opposed to using the same type of balloon as in previous works, we propose an emotion-aware balloon generation method to create different types of word balloons by analyzing the emotion of subtitles and audios. Our method is able to vary balloon shapes and word sizes in balloons in response to different emotions, leading to more enriched reading experience. Once the balloons are generated, they are placed adjacent to their corresponding speakers via speaker detection. Our results show that our method, without requiring any user inputs, can generate high-quality comic pages with visually rich layouts and balloons. Our user studies also demonstrate that users prefer our generated results over those by state-of-the-art comic generation systems."
          },
          {
            authors: 'Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, Xin Yang, Baocai Yin.',
            title: 'Reweighted Non-convex Non-smooth Rank Minimization based Spectral Clustering on Grassmann Manifold.',
            tags: [
              ['The 15th Asian Conference on Computer Vision (ACCV) 2020', 1],
              [', Kyoto, Japan.', 0]
            ],
            imgs: [
              { src: 'Reweighted Non-convex Non-smooth Rank Minimization based Spectral Clustering on Grassmann Manifold2.png' }
            ],
            abstract: "Low Rank Representation (LRR) based unsupervised clustering methods have achieved great success since these methods could explore low-dimensional subspace structure embedded in original data effectively. The conventional LRR methods generally treat the data as the points in Euclidean space. However, it is no longer suitable for high-dimension data (such as video or imageset). That is because high-dimension data are always considered as non-linear manifold data such as Grassmann manifold. Besides, the typical LRR methods always adopt the traditional single nuclear norm based low rank constraint which cannot fully reveal the low rank property of the data representation and often leads to suboptimal solution. In this paper, a new LRR based clustering model is constructed on Grassmann manifold for high-dimension data. In the proposed method, each high-dimension data is formed as a sample on Grassmann manifold with non-linear metric. Meanwhile, a non-convex low rank representation is adopt to reveal the intrinsic property of these high-dimension data and reweighted rank minimization constraint is introduced. The experimental results on several publicdatasets show that the proposed method outperforms the state-of-the-art clustering methods."
          },
          {
            authors: 'Hengfeng Zha, Dongsheng Zhou, Xin Yang,Qiang Zhang,Xiaopeng Wei.',
            title: 'Reweighted Non-convex Non-smooth Rank Minimization based Spectral Clustering on Grassmann Manifold.',
            tags: [
              ['The 12th Asian Conference on Machine Learning (ACML) 2020', 1],
              ['.', 0]
            ],
            imgs: [
              { src: 'Reweighted Non-convex Non-smooth Rank Minimization based Spectral Clustering on Grassmann Manifold.png' }
            ],
            abstract: 'In recent years, the attention mechanism has been widely used in computer vision. Semantic segmentation, as one of the fundamental tasks of computer vision, has been subject to tremendous development as a result. But because of its huge computing overhead, attention-based approaches are difficult to use for real-time applications such as self-driving. In this paper, we propose a self-calibration method baesd on self-attentiion that successfully applies the attention mechanism to real-time semantic segmentation. Specifically, a spatial attention module to adjust the edges of the coarse segmentation results which gained from the real-time semantic segmentation backbone network, and obtain more granular segmentation results. We refer to this method as the Efficient Attentional Calibration Network (EACNet). Experiments on the Cityscapes dataset validate the efficiency and performance of the method. With the high-resolution input and without any post-processing, EACNet achieved 72.4% mIoU of accuracy while running at 116.9 FPS. Compared to other state-of-the-art methods for real-time semantic segmentation, our network gained a better balance between performance and speed.'
          },

          {
            authors: 'Lei Chen, Rui Liu, Dongsheng Zhou, Yang Xin, Qiang Zhang and Xiaopeng Wei.',
            title: 'A Human Behavior Recognition Model based on Extended Squeeze-and-Excitation Network.',
            tags: [
              ['CASA 2020', 1],
              [', Bournemouth, UK. ', 0]
            ],
            imgs: [
              { src: 'A Human Behavior Recognition Model based on Extended Squeeze-and-Excitation Network.png' }
            ],
            abstract: 'Human behavior recognition in video sequences is an important research problem in computer vision, and has attracted a lot of researchersâ€™ attention in recent years. Because of its powerful ability in feature channels recalibration, the Squeeze-and-Excitation network (SE network) has been widely used in video behavior recognition models. However, most of these methods use the SE network directly to recalibrate different feature channels of a single frame image, but ignore the temporal dimension of videos. To address this problem, a new behavior recognition model based on extended Squeeze-and-Excitation network (named as ESENet) is proposed in this study. In order to improve the attention capability, the integration strategy of the 3DSE is also discussed in this study. Experiment results show that the proposed method in this paper achieves competitive performance on the challenging HMDB51, UCF101, and Something-Something v1 datasets.'
          },
          {
            authors:
              'Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, Yanwei Pang.',
            title: 'Attention Scaling for Crowd Counting.',
            tags: [
              ['CVPR 2020', 1],
              [', Seattle, USA. ', 0],
              ['(CCF A)', 2]
            ],
            links: [
              { name: 'paper', link: 'http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Attention_Scaling_for_Crowd_Counting_CVPR_2020_paper.pdf', },
              { name: 'code', link: 'https://github.com/laridzhang/ASNet' },
            ],
            imgs: [
              { src: 'Attention Scaling for Crowd Counting.png' }
            ],
            abstract: 'Convolutional Neural Network (CNN) based methods generally take crowd counting as a regression task by outputting crowd densities. They learn the mapping between image contents and crowd density distributions. Though having achieved promising results, these data-driven counting networks are prone to overestimate or underestimate people counts of regions with different density patterns, which degrades the whole count accuracy. To overcome this problem, we propose an approach to alleviate the counting performance differences in different regions. Specifically, our approach consists of two networks named Density Attention Network (DANet) and Attention Scaling Network (ASNet). DANet provides ASNet with attention masks related to regions of different density levels. ASNet first generates density maps and scaling factors and then multiplies them by attention masks to output separate attention-based density maps. These density maps are summed to give the final density map. The attention scaling factors help attenuate the estimation errors in different regions. Furthermore, we present a novel Adaptive Pyramid Loss (APLoss) to hierarchically calculate the estimation losses of sub-regions, which alleviates the training bias. Extensive experiments on four challenging datasets (ShanghaiTech Part A, UCF_CC_50, UCF-QNRF, and WorldExpo\'10) demonstrate the superiority of the proposed approach.'
          },
          {
            authors:
              'Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, Xin Yang, Baocai Yin, Wenwu Zhu and Ge Li.',
            title:
              'Kernel Clustering on Symmetric Positive Definite Manifolds via Double Approximated Low Rank Representation.',
            tags: [['ICME 2020', 1], [', London, UK. ', 0], ['(CCF B)', 2]],
            links: [
              {
                name: 'paper',
                link: 'https://ieeexplore.ieee.org/document/9102821',
              },
            ],
            imgs: [
              { src: 'Kernel Clustering on Symmetric Positive Definite Manifolds via Double Approximated Low Rank Representation.png' }
            ],
            abstract: 'As an effective descriptor, Symmetric Positive Definite (SPD) matrix is widely used in several areas such as image clustering. Recently, researchers proposed some effective methods based on low rank theory for SPD data clustering with nonlinear metric. However, single nuclear norm is always adopted to formulate the low rank model in these methods, which would lead to suboptimal solution. In this paper, we proposed a novel double low rank representation method for SPD clustering problem, in which matrix factorization and nonconvex rank constraint are combined to reveal the intrinsic property of the data instead of employing the nuclear norm. Meanwhile, kernel method and Log-Euclidean metric are combined to better explore the intrinsic geometry within SPD data. The proposed method has been evaluated on several public datasets and the experimental results demonstrate that the proposed method outperforms the state-of-the-art ones.'
          },
          {
            authors:
              'Tianyu Wang*, Xin Yang*, Ke Xu, Shaozhe Chen, Qiang Zhang, and Rynson Lau.',
            title:
              'Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset.',
            tags: [
              ['CVPR 2019', 1],
              [', Long Beach, USA. ', 0],
              ['(CCF A)', 2]
            ],
            links: [
              {
                name: 'paper',
                link: 'https://arxiv.org/abs/1904.01538',
              },
              {
                name: 'suppl',
                link:
                  'http://www.cs.cityu.edu.hk/~rynson/papers/demos/cvpr19c-supp.pdf',
              },
              {
                name: 'code',
                link: 'https://github.com/stevewongv/SPANet',
              },
              {
                name: 'training-set (key:4fwo)',
                link: 'https://pan.baidu.com/s/1lPn3MWckHxh1uBYYucoWVQ',
              },
              {
                name: 'test-set',
                link:
                  'http://www.cs.cityu.edu.hk/~rynson/papers/demos/derain_testset_1000.rar',
              },
            ],
            imgs: [
              { src: 'Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset.png' },
              { src: 'Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset.gif' },
              { src: 'Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset (2).gif' }
            ],
            abstract: 'Removing rain streaks from a single image has been drawing considerable attention as rain streaks can severely degrade the image quality and affect the performance of existing outdoor vision tasks. While recent CNN-based derainers have reported promising performances, deraining remains an open problem for two reasons. First, existing synthesized rain datasets have only limited realism, in terms of modeling real rain characteristics such as rain shape, direction and intensity. Second, there are no public benchmarks for quantitative comparisons on real rain images, which makes the current evaluation less objective. The core challenge is that real world rain/clean image pairs cannot be captured at the same time. In this paper, we address the single image rain removal problem in two ways. First, we propose a semi-automatic method that incorporates temporal priors and human supervision to generate a high-quality clean image from each input sequence of real rain images. Using this method, we construct a large-scale dataset of âˆ¼29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes. Second, to better cover the stochastic distribution of real rain streaks, we propose a novel SPatial Attentive Network (SPANet) to remove rain streaks in a local-to-global manner. Extensive experiments demonstrate that our network performs favorably against the state-of-the-art deraining methods.'
          },
        ],
      }
    },
    methods: {
      textBold(s) {
        return textBold(s)
      },
      goBack() {
        goBack()
      },
    },
  })
</script>

</html>