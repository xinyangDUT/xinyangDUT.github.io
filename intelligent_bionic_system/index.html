<!DOCTYPE html>
<html>
<title>Bio-inspired Intelligence and Robotic Systems</title>

<head>
  <meta charset="UTF-8" />
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="../css/element-ui.css" />
  <link rel="stylesheet" href="../css/index.css" />
  <script src="../js/vue.min.js"></script>
  <script src="../js/element-ui.min.js"></script>
</head>

<body>
  <div id="app">
    <el-page-header @back="goBack" title="Back"> </el-page-header>
    <div>
      <h1>Bio-inspired Intelligence and Robotic Systems</h1>
      <el-divider></el-divider>
      <p>
        The research interests of the biologically-inspired intelligence lie at the intersection of computer vision,
        robotics and biology. Through the abstraction of the design principles of biological systems, we explore the
        applications of bio-inspired sensors (Event-based Camera), bio-inspired algorithms (Spiking Neural Networks),
        and bio-inspired hardware in robotic systems. Our main goals are to engineer novel robotic applications which
        are more adaptive, maneuverable, resilient and energy-efficient through a deeper understanding of adaptivity and
        autonomy of biology.
      </p>
      <el-divider></el-divider>
      <div v-for="project,index in projects">
        <p class="project_p">
          <span v-html="textBold(project.authors)"></span>
          <b class="project_title" :id="index">{{project.title}}</b>
          <span v-for="tag in project.tags" :style="styles[tag[1]]">{{tag[0]}}</span>
          <span v-for="link in project.links">[<b><a class="project_links"
                :href="link.link">{{link.name}}</a></b><span>]&nbsp;</span></span>
        </p>
        <div v-if="project.video" class="figure_div">
          <video width="640" :src="'videos/'+project.video" controls></video>
        </div>
        <div v-for="img in project.imgs" class="figure_div">
          <img :src="'figures/'+img" :alt="img" class="figure_img" />
          <div v-if="img.label">
            <div class="figure_label">{{img.label}}</div>
            <br>
          </div>
        </div>
        <div v-if="project.io">
          <p><b>Input-Output:&nbsp;</b>{{project.io}}<span></span></p>
        </div>
        <div v-if="project.abstract">
          <p><b>Abstract.&nbsp;</b>{{project.abstract}}<span></span></p>
        </div>
        <el-divider content-position="right">{{index+1}}</el-divider>
      </div>
    </div>
  </div>
</body>
<script src="../js/index.js"></script>
<script>
  new Vue({
    el: '#app',
    data: function () {
      return {
        styles,
        projects: [
   
        {
            authors: 'Yang Wang, Bo Dong, Yuji Zhang, Yunduo Zhou, Haiyang Mei, Ziqi Wei, Xin Yang. ',
            title: 'Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle Avoidance.',
            io: ' Given the event data, laser data, goal and velocity, our framework produces the action of robot.',
            tags: [
              ['ACM MM2023', 1],
              ['. ', 0],
              ['(CCF A)', 2]

            ],
            abstract: 'Autonomous obstacle avoidance is of vital importance for an intelligent agent such as a mobile robot to navigate in its environment. Existing state-of-the-art methods train a spiking neural network (SNN) with deep reinforcement learning (DRL) to achieve energy-efficient and fast inference speed in complex/unknown scenes. These methods typically assume that the environment is static while the obstacles in real-world scenes are often dynamic. The movement of obstacles increases the complexity of the environment and poses a great challenge to the existing obstacle avoidance methods. In this work, we approach robust dynamic obstacle avoidance twofold. First, we introduce the neuromorphic vision sensor (i.e., event camera) to provide motion cues complementary to the traditional Laser depth data for handling dynamic obstacles. Second, we develop an DRL-based event-enhanced multimodal spiking actor network (EEM-SAN) that extracts information from motion events data via unsupervised representation learning and fuses Laser and event camera data with learnable thresholding. Experimental results demonstrate that our EEM-SAN outperforms state-of-the-art obstacle avoidance methods by a significant margin, especially for dynamic obstacle avoidance.',
            imgs: [
              'Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle Avoidance.png',
            ],
          },
        {
            authors: 'Haiwei Zhang, Jiqing Zhang, Bo Dong, Pieter Peers, Wenwei Wu, Xiaopeng Wei, Felix Heide, Xin Yang. ',
            title: 'In the Blink of an Eye: Event-based Emotion Recognition.',
            io: ' Give an single-eye-based emotion squence, including intensity and corresponding events, our framework produces the emotion recognition classification results.',
            tags: [
              ['SIGGRAPH 2023', 1],
              ['. ', 0],
              ['(CCF A)', 2]

            ],
            video: 'Event-based Emotion Recognition.mp4',
            abstract: 'In this paper, we introduce a wearable single-eye emotion recognition setup and a real-time approach for recognizing emotions from partial observations of the emotion event that is robust to changes in lighting conditions. Key to our method is a bio-inspired event-based camera setup and a newly designed lightweight Spiking Eye Emotion Network (SEEN). Compared to conventional cameras, event-based cameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher temporal resolution (in the order of ms vs. 10s of ms). Thus, the captured events can encode rich temporal cues under challenging lighting conditions. However, these events lack texture information, posing problems in effectively decoding the temporal information. SEEN effectively tackles the issue from two different perspectives. First, we adopt convolutional spiking layers to take advantage of the ability of the spiking neuron networks ability to decode pertinent temporal information. Second, SEEN learns to extract essential spatial cues from corresponding intensity frames and leverages a novel-designed weight-copy scheme to effectively convey spatial attention to the convolutional spiking layers during training and inference. We extensively validate and demonstrate the effectiveness of our approach on a specially collected Single-eye Event-based Emotion (SEE) dataset.To the best of our knowledge,our method is the first eye-based emotion recognition method that leverages event-based cameras and spiking neural networks.',
            imgs: [
              'In the blink of an eye-1.png',
              'In the blink of an eye-2.png',
            ],
          },
        {
            authors: 'Yang Wang, Bo Dong, Ke Xu, Haiyin Piao, Yufei Ding, Baocai Yin, Xin Yang†. ',
            title: 'A Geometrical Approach to Evaluate the Adversarial Robustness of Deep Neural Networks.',
            io: ' Give an input image and a adversarial attack method, our network outputs a score that indicates how image robustness is.',
            tags: [
              ['ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 2023.', 1],
              ['. ', 0]
            ],
            abstract: 'Deep Neural Networks (DNNs) are widely used for computer vision tasks. However, it has been shown that deep models are vulnerable to adversarial attacks, i.e., their performances drop when imperceptible perturbations are made to the original inputs, which may further degrade the following visual tasks or introduce new problems such as data and privacy security. Hence, metrics for evaluating the robustness of deep models against adversarial attacks are desired. However, previous metrics are mainly proposed for evaluating the adversarial robustness of shallow networks on the small-scale datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet dataset), it is computationally expensive and its performance relies on a tractable number of samples. In this paper, we propose the Adversarial Converging Time Score (ACTS), an attack-dependent metric that quantifies the adversarial robustness of a DNN on a specific input. Our key observation is that local neighborhoods on a DNN’s output surface would have different shapes given different inputs. Hence, given different inputs, it requires different time for converging to an adversarial sample. Based on this geometry meaning, ACTS measures the converging time as an adversarial robustness metric. We validate the effectiveness and generalization of the proposed ACTS metric against different adversarial attacks on the large-scale ImageNet dataset using state-of-the-art deep networks. Extensive experiments show that our ACTS metric is an efficient and effective adversarial metric over the previous CLEVER metric.',
            imgs: [
              'Deep Neural Networks-1.png',
              'Deep Neural Networks-2.png',
              'Deep Neural Networks-3.png'
            ],
          },

        {
            authors: 'Boyan Wei, Xianfeng Ye, Chengjiang Long, Zhenjun Du, Bangyu Li, Baocai Yin, Xin Yang*.',
            title: 'Discriminative Active Learning for Robotic Grasping in Cluttered Scene.',
            io: 'RGB-D images, our framework produces object 4D grasp pose.',
            tags: [
              ['IEEE Robotics and Automation Letters (RA-L) 2023', 1],
              ['. ', 0]
            ],
            video: 'RAL.mp4',
            abstract: 'Robotic grasping is a challenging task due to the diversity of object shapes. A sufficiently labeled dataset is essential for the grasp pose detection methods based on deep learning. However, data annotation is a costly procedure. Active learning aims to mitigate the greedy need for massive labeled data. In this work, we propose a Discriminative Active Learning (DAL) framework for robotic grasping algorithms. DAL is an effective strategy that utilizes a shared encoder to derive latent features from both labeled data and unlabeled data. A discriminator is established to estimate the informativeness of each unlabeled data sample and decide whether they should be annotated for the next epoch. Moreover, an annotation interface is also developed to annotate the chosen data. We evaluate DAL with real-world grasp datasets and show superior performance, especially when the amount of labeled data is little. Considering annotation noise, we perform an experiment on a noisy dataset and demonstrate that our proposed framework is stable to noisy annotation. Besides, we train a model with about 60\% data selected by DAL of the whole dataset and it can still handle a real-world grasp detection task in cluttered scene on a real robot.',
            imgs: [
              'RAL-1.png'
            ],
            links: [
              { name: 'paper', link: './DAL-final.pdf' }
            ]
          },
          {
            authors: 'Jianchuan Ding, Bo Dong, Felix Heide, Yufei Ding, Yunduo Zhou, Baocai Yin, Xin Yang*.',
            title: 'Biologically Inspired Dynamic Thresholds for Spiking Neural Networks.',
            io: 'Given the laser data, goal and velocity, our framework produces the robot\'s action.',
            tags: [
              ['NeurIPS 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            video: 'Biologically-Inspired.mp4',
            abstract: 'The dynamic membrane potential threshold, as one of the essential properties of a biological neuron, is a spontaneous regulation mechanism that maintains neuronal homeostasis, i.e., the constant overall spiking firing rate of a neuron. As such, the neuron firing rate is regulated by a dynamic spiking threshold, which has been extensively studied in biology. Existing work in the machine learning community does not employ bioinspired spiking threshold schemes. This work aims at bridging this gap by introducing a novel bioinspired dynamic energy-temporal threshold (BDETT) scheme for spiking neural networks (SNNs). The proposed BDETT scheme mirrors two bioplausible observations: a dynamic threshold has 1) a positive correlation with the average membrane potential and 2) a negative correlation with the preceding rate of depolarization. We validate the effectiveness of the proposed BDETT on robot obstacle avoidance and continuous control tasks under both normal conditions and various degraded conditions, including noisy observations, weights, and dynamic environments. We find that the BDETT outperforms existing static and heuristic threshold approaches by significant margins in all tested conditions, and we confirm that the proposed bioinspired dynamic threshold scheme offers homeostasis to SNNs in complex real-world tasks.',
            imgs: [
              'Biologically-Inspired-2.png'
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2206.04426.pdf' }
            ]
          },
          {
            authors: 'Hu Lin, Meng Li, Qianchen Xia, Yifeng Fei, Baocai Yin, Xin Yang.',
            title: '6-DoF Pose Relocalization for Event Cameras With Entropy Frame and Attention Networks.',
            io: ' Given the event stream, our network produces the 6DoF camera pose estimation.',
            tags: [
              ['ACM SIGGRAPH VRCAI 2022', 1],
              ['. ', 0]
            ],
            video: 'vrcai.mp4',
            abstract: 'Camera relocalization is an important task in computer vision, mainly used in applications such as VR, AR, and robotics. Camera relocalization solves the problem of estimating the 6-DoF camera pose of an input image in a known scene. There are large numbers of research on standard cameras. However, standard cameras have problems such as large power consumption, low frame rate, and poor robustness. Event cameras can make up for the disadvantages of standard cameras. Event data is different from RGB data, it is asynchronous streaming data, most of the processing methods for events convert event data into event images, but these methods can not efficiently generate event images with clear edges at any time, we propose a Reversed Window Entropy Image (RWEI) generation framework for events, which can generate event images with clear edges at any time. We also propose an Attention-guided Event Camera Relocalization Network (AECRN) for utilizing event image characteristics to estimate the pose of the event camera more accurately. We demonstrate our proposed framework and network on public dataset sequences, and experiments show that our proposed method surpasses the previous method.',
            imgs: [
              'vrcai-1.png',
              'vrcai-2.png'
            ],
            links: [
              { name: 'paper', link: 'https://doi.org/10.1145/3574131.3574457' },
              { name: 'code', link: 'https://github.com/linharrrrrt/RWEI_AECRN' }
            ]
          },
          {
            authors: 'Jianchuan Ding, Lingping Gao, Wenxi Liu, Haiyin Piao, Jia Pan, Zhenjun Du, Xin Yang*, Baocai Yin.',
            title: 'Monocular Camera-based Complex Obstacle Avoidance via Efficient Deep Reinforcement Learning.',
            io: 'Given the monocular RGB image, goal and velocity, our framework produces the robot\'s action.',
            tags: [
              ['IEEE Transactions on Circuits and Systems for Video Technology 2022', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            abstract: 'Deep reinforcement learning has achieved great success in laser-based collision avoidance works because the laser can sense accurate depth information without too much redundant data, which can maintain the robustness of the algorithm when it is migrated from the simulation environment to the real world. However, high-cost laser devices are not only difficult to deploy for a large scale of robots but also demonstrate unsatisfactory robustness towards the complex obstacles, including irregular obstacles, e.g., tables, chairs, and shelves, as well as complex ground and special materials. In this paper, we propose a novel monocular camera-based complex obstacle avoidance framework. Particularly, we innovatively transform the captured RGB images to pseudo-laser measurements for efficient deep reinforcement learning. Compared to the traditional laser measurement captured at a certain height that only contains one-dimensional distance information away from the neighboring obstacles, our proposed pseudo-laser measurement fuses the depth and semantic information of the captured RGB image, which makes our method effective for complex obstacles. We also design a feature extraction guidance module to weight the input pseudo-laser measurement, and the agent has more reasonable attention for the current state, which is conducive to improving the accuracy and efficiency of the obstacle avoidance policy.',
            video: 'Monocular.mp4',
            imgs: [
              'Monocular-Camera-based-2.png'
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2209.00296.pdf' }
            ]
          },
          {
            authors: 'Jiqing Zhang, Bo Dong, Haiwei Zhang, Jianchuan Ding, Felix Heide, Baocai Yin, Xin Yang*.',
            title: 'Spiking Transformers for Event-based Single Object Tracking.',
            io: 'Given the first event frame image and corresponding bounding box, our network produces the following bounding boxes.',
            tags: [
              ['CVPR 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            abstract: 'Event-based cameras bring a unique capability to tracking, being able to function in challenging real-world conditions as a direct result of their high temporal resolution and high dynamic range. These imagers capture events asynchronously that encode rich temporal and spatial information. However, effectively extracting this information from events remains an open challenge. In this work, we propose a spiking transformer network, STNet, for single object tracking. STNet dynamically extracts and fuses information from both temporal and spatial domains. In particular, the proposed architecture features a transformer module to provide global spatial information and a spiking neural network (SNN) module for extracting temporal cues. The spiking threshold of the SNN module is dynamically adjusted based on the statistical cues of the spatial information, which we find essential in providing robust SNN features. We fuse both feature branches dynamically with a novel cross-domain attention fusion algorithm. Extensive experiments on three event-based datasets, FE240hz, EED and VisEvent validate that the proposed STNet outperforms existing state-of-the-art methods in both tracking accuracy and speed with a significant margin. The code and pretrained models are at https://github.com/Jee-King/CVPR2022_STNet.',
            imgs: [
              'SpikingTransformer-video_v2.gif',
              'Spiking Transformers.png'
            ],
            links: [
              { name: 'code', link: 'https://github.com/Jee-King/CVPR2022_STNet' },
              { name: 'demo', link: 'https://youtu.be/iRkgBy0V1Dk' }
            ],
          },
          {
            authors: '朱强, 王超毅, 张吉庆, 尹宝才, 魏小鹏, 杨鑫*.',
            title: '基于事件相机的无人机目标跟踪.',
            io: '给定成对的灰度图像和事件图像以及第一帧目标的位置，预测后续帧目标的位置.',
            tags: [
              ['浙江大学学报（理学版）', 1],
              ['. 2022, 49(01).', 0]
            ],
            abstract: '无人机的目标跟踪现如今已成为计算机视觉领域中一个热门的研究课题. 无人机目标跟踪可以应用于消防、军事等重要领域.现阶段无人机目标跟踪算法大多基于传统的 RGB 相机结合深度学习算法, 但这类方法一方面无法避免因无人机自身机体抖动引发的运动模糊问题, 另一方面传统 RGB 相机在低光照或过曝光场景下成像较差, 从而难以追踪目标. 为了解决上述无人机视角下的目标跟踪出现的问题，本文采用无人机搭载 DAVIS 事件相机进行目标跟踪, 并设计了基于事件与灰度图的双模态融合跟踪网络，为了更好地训练该网络，本文采用运动捕捉系统Vicon自行制作无人机视角下的目标跟踪数据集：Event-APS28，保证了复杂光照场景下可以根据图像信息有效的跟踪目标.',
            imgs: [
              '基于事件相机的无人机目标跟踪.png'
            ]
          },
          {
            authors: 'Jiqing Zhang, Xin Yang, Yingkai Fu, Xiaopeng Wei, Baocai Yin, Bo Dong.',
            title: 'Object Tracking by Jointly Exploiting Frame and Event Domain.',
            io: 'Given the first frame image, and it\'s corresponding events and bounding box, our network produces the following bounding boxes.',
            tags: [
              ['ICCV 2021', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            abstract: 'Inspired by the complementarity between conventional frame-based and bio-inspired event-based cameras, we propose a multi-modal based approach to fuse visual cues from the frame- and event-domain to enhance the single object tracking performance, especially in degraded conditions (e.g., scenes with high dynamic range, low light, and fast motion objects). The proposed approach can effectively and adaptively combine meaningful information from both domains. Our approach’s effectiveness is enforced by a novel designed cross-domain attention schemes, which can effectively enhance features based on self- and cross-domain attention schemes; The adaptiveness is guarded by a specially designed weighting scheme, which can adaptively balance the contribution of the two domains. To exploit event-based visual cues in single-object tracking, we construct a largescale frame-event-based dataset, which we subsequently employ to train a novel frame-event fusion based model. Extensive experiments show that the proposed approach outperforms state-of-the-art frame-based tracking methods by at least 10.4% and 11.9% in terms of representative success rate and precision rate, respectively. Besides, the effectiveness of each key component of our approach is evidenced by our thorough ablation study.',
            imgs: [
              'Object-Tracking-demo.gif',
              'Object-Tracking-1.png'
            ],
            links: [
              { name: 'media', link: 'https://youtu.be/EeMRO8XVv04' },
              { name: 'code', link: 'https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking' },
              { name: 'dataset', link: 'https://zhangjiqing.com/dataset/' }
            ],
          },
          {
            authors: 'Jiqing Zhang, Kai Zhao, Bo Dong, Yingkai Fu, Xinglin Piao, Xin Yang, Baocai Yin.',
            title: 'Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking.',
            io: 'Given the first frame image, and it\'s corresponding events and bounding box, our network produces the following bounding boxes.',
            tags: [
              ['The Visual Computer (Proc. CGI 2021)', 1],
              ['. ', 0]
            ],
            abstract: 'Jointly exploiting multiple different yet complementary domain information has been proven to be an effective way to perform robust object tracking. This paper focuses on effectively representing and utilizing complementary features from the frame domain and event domain for boosting object tracking performance in challenge scenarios. Specifically, we propose Common Features Extractor (CFE) to learn potential common representations from the RGB domain and event domain. For learning the unique features of the two domains, we utilize a Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract edge cues in the event domain which may be missed in RGB in some challenging conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional Neural Networks to extract texture and semantic information in RGB domain. Extensive experiments on standard RGB benchmark and real event tracking dataset demonstrate the effectiveness of the proposed approach. We show our approach outperforms all compared state-of-the-art tracking algorithms and verify event-based data is a powerful cue for tracking in challenging scenes.',
            imgs: [
              'Multi-domain-1.png',
              'Multi-domain-2.png'
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2108.04521.pdf' },
              { name: 'media', link: 'https://www.youtube.com/watch?v=NrsXMJYSDDQ&t=4s' }
            ]
          },
          {
            authors: 'Lingping Gao, Jianchuan Ding, Wenxi Liu, Haiyin Piao, Yuxin Wang, Xin Yang†, Baocai Yin.',
            title: 'A Vision-based Irregular Obstacle Avoidance Framework via Deep Reinforcement Learning.',
            io: 'Given the monocular RGB image, goal and velocity, our framework produces the robot\'s action.',
            tags: [
              ['IROS 2021', 1],
              ['. ', 0]
            ],
            abstract: 'Deep reinforcement learning has achieved great success in laser-based collision avoidance work because the laser can sense accurate depth information without too much redundant data, which can maintain the robustness of the algorithm when it is migrated from the simulation environment to the real world. However, high-cost laser devices are not only difficult to apply on a large scale but also have poor robustness to irregular objects, e.g., tables, chairs, shelves, etc. In this paper, we propose a vision-based collision avoidance framework to solve the challenging problem. Our method attempts to estimate the depth and incorporate the semantic information from RGB data to obtain a new form of data, pseudo-laser data, which combines the advantages of visual information and laser information. Compared to traditional laser data that only contains the one-dimensional distance information captured at a certain height, our proposed pseudo-laser data encodes the depth information and semantic information within the image, which makes our method more effective for irregular obstacles. Besides, we adaptively add noise to the laser data during the training stage to increase the robustness of our model in the real world, due to the estimated depth information is not accurate. Experimental results show that our framework achieves state-of-the-art performance in several unseen virtual and real-world scenarios.',
            imgs: [
              'Obstacle-Avoidance-demo.gif',
              'Obstacle-Avoidance-1.png'
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2108.06887.pdf' }
            ]
          },
        ]
      }
    },
    methods: {
      textBold(s) {
        return textBold(s)
      },
      goBack() {
        goBack()
      },
    },
  })
</script>

</html>