<!DOCTYPE html>
<html>
<title>Uncertain Object Detection/Segmentation</title>

<head>
  <meta charset="UTF-8" />
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="../css/element-ui.css" />
  <link rel="stylesheet" href="../css/index.css" />
  <script src="../js/vue.min.js"></script>
  <script src="../js/element-ui.min.js"></script>
</head>

<body>
  <div id="app">
    <el-page-header @back="goBack" title="Back"> </el-page-header>
    <!-- Uncertain Object Detection/Segmentation -->
    <div class="content">
      <h1>Uncertain Object Detection/Segmentation</h1>
      <el-divider></el-divider>
      <p>
        The uncertain scene object refers to the object which brings uncertainty to the scene understanding process for
        intelligence systems. The presence of uncertain objects in the scene severely affects intelligent decisions in
        many applications such as robotic navigation and drone tracking. In this project, we are developing techniques
        for uncertain object detection/segmentation. Based on environmental data perception, we are committed to mining
        the features and patterns of scene data from different perspectives such as computational statistics, behavioral
        cognition, and semantics, combining multi-dimensional and multi-modal technical means, so as to realize
        effective analysis, cognition, and expression of the uncertain scene object. Specifically, the uncertain objects
        we focus on include special, camouflaged, and salient objects in the scene.
      </p>
      <el-divider></el-divider>
      <div v-for="project,index in projects">
        <p class="project_p">
          <span v-html="textBold(project.authors)"></span>
          <b class="project_title">{{project.title}}</b>
          <span v-for="tag in project.tags" :style="styles[tag[1]]">{{tag[0]}}</span>
          <span v-for="link in project.links">[<b><a class="project_links"
                :href="link.link">{{link.name}}</a></b><span>]&nbsp;</span></span>
        </p>
        <div v-for="img in project.imgs" class="figure_div">
          <img :src="'figures/'+img.src" :alt="img" class="figure_img" />
          <div v-if="img.label">
            <div class="figure_label">{{img.label}}</div>
            <br />
          </div>
        </div>
        <div v-if="project.io">
          <p><b>Input-Output:&nbsp;</b>{{project.io}}<span></span></p>
        </div>
        <div v-if="project.abstract">
          <p><b>Abstract.&nbsp;</b>{{project.abstract}}<span></span></p>
        </div>
        <el-divider content-position="right">{{index+1}}</el-divider>
      </div>
    </div>
  </div>
</body>
<script src="../js/index.js"></script>
<script>
  new Vue({
    el: "#app",
    data: function () {
      return {
        styles,
        projects: [
          {
            authors: 'Haiyang Mei, Bo Dong, Wen Dong, Jiaxi Yang, Seung-Hwan Baek, Felix Heide, Pieter Peers, Xiaopeng Wei, Xin Yang.',
            title: 'Glass Segmentation using Intensity and Spectral Polarization Cues.',
            tags: [
              ['CVPR 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given the input RGB and trichromatic polarization images, our network outputs a binary mask that indicates where transparent glass regions are.',
            abstract: 'Transparent and semi-transparent materials pose significant challenges for existing scene understanding and segmentation algorithms due to their lack of RGB texture which impedes the extraction of meaningful features. In this work, we exploit that the light-matter interactions on glass materials provide unique intensity-polarization cues for each observed wavelength of light. We present a novel learning-based glass segmentation network that leverages both trichromatic (RGB) intensities as well as trichromatic linear polarization cues from a single photograph captured without making any assumption on the polarization state of the illumination. Our novel network architecture dynamically fuses and weights both the trichromatic color and polarization cues using a novel global-guidance and multiscale self-attention module, and leverages global cross-domain contextual information to achieve robust segmentation. We train and extensively validate our segmentation method on a new large-scale RGB-Polarization dataset (RGBP-Glass), and demonstrate that our method outperforms state-of-the-art segmentation approaches by a significant margin.',
            imgs: [
              { src: 'Glass Segmentation using Intensity.gif' }
            ],
            links: [
              { name: 'paper', link: 'https://openaccess.thecvf.com/content/CVPR2022/papers/Mei_Glass_Segmentation_Using_Intensity_and_Spectral_Polarization_Cues_CVPR_2022_paper.pdf' },
              { name: 'code', link: 'https://mhaiyang.github.io/CVPR2022_PGSNet/index.html' }
            ]
          },
          {
            authors: 'Xin Tian, Ke Xu, Xin Yang, Lin Du, Baocai Yin, Rynson W.H. Lau.',
            title: 'Bi-directional Object-Context Prioritization Learning for Saliency Ranking.',
            tags: [
              ['CVPR 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input RGB image, our network outputs the saliency ranking results.',
            abstract: 'The saliency ranking task is recently proposed to study the visual behavior that humans would typically shift their attention over different objects of a scene based on their degrees of saliency. Existing approaches focus on learning either object-object or object-scene relations. Such a strategy follows the idea of object-based attention in Psychology, but it tends to favor objects with strong semantics (e.g., humans), resulting in unrealistic saliency ranking. We observe that spatial attention works concurrently with object-based attention in the human visual recognition system. During the recognition process, the human spatial attention mechanism would move, engage, and disengage from region to region (i.e., context to context). This inspires us to model region-level interactions, in addition to object-level reasoning, for saliency ranking. Hence, we propose a novel bidirectional method to unify spatial attention and object-based attention for saliency ranking. Our model has two novel modules: (1) a selective object saliency (SOS) module to model object-based attention via inferring the semantic representation of salient objects, and (2) an object-context-object relation (OCOR) module to allocate saliency ranks to objects by jointly modeling object-context and context-object interactions of salient objects. Extensive experiments show that our approach outperforms existing state-of-the-art methods.',
            imgs: [
              { src: 'Bi-directional-1.png' },
              { src: 'Bi-directional-2.png' }
            ],
            links: [
              { name: 'paper', link: 'https://openaccess.thecvf.com/content/CVPR2022/papers/Tian_Bi-Directional_Object-Context_Prioritization_Learning_for_Saliency_Ranking_CVPR_2022_paper.pdf' },
              { name: 'code', link: 'https://github.com/GrassBro/OCOR' }
            ]
          },
          {
            authors: 'Letian Yu*, Haiyang Mei*, Wen Dong, Ziqi Wei, Li Zhu, Yuxin Wang, and Xin Yang†.',
            title: 'Progressive Glass Segmentation.',
            tags: [
              ['IEEE Transactions on Image Processing 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input RGB image, our network outputs a binary mask that indicates where transparent glass regions are.',
            abstract: 'Glass is very common in the real world. Influenced by the uncertainty about the glass region and the varying complex scenes behind the glass, the existence of glass poses severe challenges to many computer vision tasks, making glass segmentation as an important computer vision task. Glass does not have its own visual appearances but only transmit/reflect the appearances of its surroundings, making it fundamentally different from other common objects. To address such a challenging task, existing methods typically explore and combine useful cues from different levels of features in the deep network. As there exists a characteristic gap between level-different features, i.e., deep layer features embed more high-level semantics and are better at locating the target objects while shallow layer features have larger spatial sizes and keep richer and more detailed low-level information, fusing these features naively thus would lead to a sub-optimal solution. In this paper, we approach the effective features fusion towards accurate glass segmentation in two steps. First, we attempt to bridge the characteristic gap between different levels of features by developing a Discriminability Enhancement (DE) module which enables level-specific features to be a more discriminative representation, alleviating the features incompatibility for fusion. Second, we design a Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful information in the fusion process by highlighting the common and exploring the difference between level-different features. Combining these two steps, we construct a Progressive Glass Segmentation Network (PGSNet) which uses multiple DE and FEBF modules to progressively aggregate features from high-level to low-level, implementing a coarse-to-fine glass segmentation. In addition, we build the first home-scene-oriented glass segmentation dataset for advancing household robot applications and in-depth research on this topic. Extensive experiments demonstrate that our method outperforms 26 cutting-edge models on three challenging datasets under four standard metrics. The code and dataset will be made publicly available.',
            imgs: [
              { src: 'Progressive Glass Segmentation_1.png' },
              { src: 'Progressive Glass Segmentation_2.png' }
            ],
            links: [
              { name: 'paper', link: 'https://ieeexplore.ieee.org/document/9748016' },
              { name: 'dataset, fetch code: hsod', link: 'https://pan.baidu.com/s/1qxXiFXYGHGPAxpLZ7h57jw' }
            ]
          },
          {
            authors: 'Haiyang Mei, Xin Yang, Letian Yu, Qiang Zhang, Xiaopeng Wei, and Rynson W.H. Lau.',
            title: 'Large-Field Contextual Feature Learning for Glass Detection.',
            tags: [
              ['IEEE Transactions on Pattern Analysis and Machine Intelligence 2022', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: ' Given an input RGB image, our network outputs a binary mask that indicates where transparent glass regions are.',
            abstract: 'Glass is very common in our daily life. Existing computer vision systems neglect it and thus may have severe consequences, e.g., a robot may crash into a glass wall. However, sensing the presence of glass is not straightforward. The key challenge is that arbitrary objects/scenes can appear behind the glass. In this paper, we propose an important problem of detecting glass surfaces from a single RGB image. To address this problem, we construct the first large-scale glass detection dataset (GDD) and propose a novel glass detection network, called GDNet-B, which explores abundant contextual cues in a large field-of-view via a novel large-field contextual feature integration (LCFI) module and integrates both high-level and low-level boundary features with a boundary feature enhancement (BFE) module. Extensive experiments demonstrate that our GDNet-B achieves satisfying glass detection results on the images within and beyond the GDD testing set. We further validate the effectiveness and generalization capability of our proposed GDNet-B by applying it to other vision tasks, including mirror segmentation and salient object detection. Finally, we show the potential applications of glass detection and discuss possible future research directions.',
            imgs: [
              { src: 'Large-Field Contextual Feature Learning for Glass Detection_1.png' },
              { src: 'Large-Field Contextual Feature Learning for Glass Detection_2.png' }
            ],
            links: [
              { name: 'paper', link: 'https://ieeexplore.ieee.org/document/9863431' },
              { name: 'code', link: 'https://mhaiyang.github.io/TPAMI2022-GDNet-B/index.html' }
            ]
          },
          {
            authors: 'Xin Tian, Ke Xu, Xin Yang†, Baocai Yin, Rynson Lau.',
            title: 'Learning to Detect Instance-level Salient Objects using Complementary Image Labels.',
            tags: [
              ['International Journal of Computer Vision 2021', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input image, our network outputs an instance saliency map that indicates the individual salient instances. It requires only class labels and subitizing information as supervision in training.',
            abstract: 'Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. As the subitizing information provides an instant judgement on the number of salient items, it is naturally related to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this observation, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is then fused to produce a salient instance map. To facilitate the learning process, we further propose a progressive training scheme to reduce label noise and the corresponding noise learned by the model, via reciprocating the model with progressive salient instance prediction and model refreshing. Our extensive evaluations show that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks.',
            imgs: [
              { src: 'ijcv_2021.jpg', label: 'The key idea of this work is to leverage complementary image-level labels (class and subitizing) to train a salient instance detection model in a weakly-supervised manner, via synergically learning to predict salient objects, detecting object boundaries and locating instance centroids.' }
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2111.10137' }
            ]
          },
          {
            authors: 'Lei Zhu, Xiaoqing Wang, Ping Li, Xin Yang, Qing Zhang, Weiming Wang, Schönlieb, Carola-Bibiane, Chen, C.L. Philip.',
            title: ' Self-supervised Self-ensembling Network for Semi-supervised RGB-D Salient Object Detection.',
            tags: [
              ['IEEE Transactions on Multimedia 2021', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            io: 'SG-CNN takes a pair of RGB and depth images as the input and predicts a saliency map and an image rotation angle.',
            abstract: 'RGB-D salient object detection aims to detect visually distinctive objects or regions from a pair of the RGB image and the depth image. State-of-the-art RGB-D saliency detectors are mainly based on convolutional neural networks but almost suffer from an intrinsic limitation relying on the labeled data, thus degrading detection accuracy in complex cases. In this work, we present a self-supervised self-ensembling network (S3Net) for semi-supervised RGB-D salient object detection by leveraging the unlabeled data and exploring a self-supervised learning mechanism. To be specific, we first build a self-guided convolutional neural network (SG-CNN) as a baseline model by developing a series of three-layer cross-model feature fusion (TCF) modules to leverage complementary information among depth and RGB modalities and formulating an auxiliary task that predicts a self-supervised image rotation angle. After that, to further explore the knowledge from unlabeled data, we assign SGCNN to a student network and a teacher network, and encourage the saliency predictions and self-supervised rotation predictions from these two networks to be consistent on the unlabeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our network quantitatively and qualitatively outperforms the state-of-the-art methods.',
            imgs: [
              { src: 'Multimedia_2021_1.png' },
              { src: 'Multimedia_2021_2.png' },
              { src: 'Multimedia_2021_3.png' }
            ]
          },
          {
            authors: 'Chengyu Zheng, Shi Ding, Mingqiang Wei, Xin Yang, Yanwen Guo, Haoran Xie, Xuefeng Yan.',
            title: 'Label Decoupling-based Three-stream Neural Network for Robust Glass Detection.',
            tags: [
              ['Computer Graphics Forum (CGF)', 1],
              [' (Special Issue of Pacific Graphics 2021, Wellington, New Zealand). ', 0],
              ['(CCF B)', 2]
            ],
            io: 'Given an input RGB image, our network outputs a binary mask that indicates where transparent glass regions are.',
            abstract: 'Most of existing object detection methods generate poor glass detection results, due to the fact that the transparent glass shares the same appearance with the background. Different from traditional wisdoms that simply use object boundary as auxiliary supervision, we exploit label decoupling to decompose the original labeled GT map into interior-diffusion map and boundary-diffusion map. The GT map in collaboration with the two generated maps breaks the imbalance distribution of object boundary. We have three key contributions to solve the transparent glass detection problem: (1) We propose a three-stream neural network to fully absorb helpful features in the three maps. (2) We design a multi-scale interactive dilation module to explore a wider range of contextual information. (3) We develop an attention-based boundary-aware feature Mosaic module to integrate multimodal information. Extensive experiments on the benchmark dataset exhibit clear improvements of our method over SOTAs in terms of overall glass detection accuracy and boundary clearness.',
            imgs: [
              { src: 'Label-Decoupling-based-1.png' },
              { src: 'Label-Decoupling-based-2.png' },
              { src: 'Label-Decoupling-based-3.png' }
            ]
          },
          {
            authors: 'Haiyang Mei, Bo Dong, Wen Dong, Pieter Peers, Qiang Zhang, Xin Yang*, Xiaopeng Wei.',
            title: 'Depth-Aware Mirror Segmentation.',
            tags: [
              ['CVPR 2021', 1],
              ['. ', 0],
              ['(CCF A, Oral)', 2]
            ],
            io: 'Given an input RGB-D image, our network outputs a binary mask that indicates where mirrors are.',
            abstract: 'We present a novel mirror segmentation method that leverages depth estimates from ToF-based cameras as an additional cue to disambiguate challenging cases where the contrast or relation in RGB colors between the mirror reflection and the surrounding scene is subtle. A key observation is that ToF depth estimates do not report the true depth of the mirror surface, but instead return the total length of the reflected light paths, thereby creating obvious depth discontinuities at the mirror boundaries. To exploit depth information in mirror segmentation, we first construct a large-scale RGB-D mirror segmentation dataset, which we subsequently employ to train a novel depth-aware mirror segmentation framework. Our mirror segmentation framework first locates the mirrors based on color and depth discontinuities and correlations. Next, our model further refines the mirror boundaries through contextual contrast taking into account both color and depth information. We extensively validate our depth-aware mirror segmentation method and demonstrate that our model outperforms state-of-the-art RGB and RGB-D based methods for mirror segmentation. Experimental results also show that depth is a powerful cue for mirror segmentation.',
            imgs: [
              { src: 'Depth-Aware.gif' }
            ],
            links: [
              { name: 'paper', link: 'https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Depth-Aware_Mirror_Segmentation_CVPR_2021_paper.pdf' },
              { name: 'link', link: 'https://mhaiyang.github.io/CVPR2021_PDNet/index.html' }
            ]
          },
          {
            authors: 'Haiyang Mei, Gepeng Ji, Ziqi Wei, Xin Yang*, Xiaopeng Wei, Dengping Fan.',
            title: 'Camouflaged Object Segmentation with Distraction Mining.',
            tags: [
              ['CVPR 2021', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input RGB image, our network outputs a binary mask that indicates where camouflaged objects are.',
            abstract: 'Camouflaged object segmentation (COS) aims to identify objects that are \'\'perfectly\'\' assimilate into their surroundings, which has a wide range of valuable applications. The key challenge of COS is that there exist high intrinsic similarities between the candidate objects and noise background. In this paper, we strive to embrace challenges towards effective and efficient COS. To this end, we develop a bio-inspired framework, termed Positioning and Focus Network (PFNet), which mimics the process of predation in nature. Specifically, our PFNet contains two key modules, i.e., the positioning module (PM) and the focus module (FM). The PM is designed to mimic the detection process in predation for positioning the potential target objects from a global perspective and the FM is then used to perform the identification process in predation for progressively refining the coarse prediction via focusing on the ambiguous regions. Notably, in the FM, we develop a novel distraction mining strategy for the distraction region discovery and removal, to benefit the performance of estimation. Extensive experiments demonstrate that our PFNet runs in real-time (72 FPS) and significantly outperforms 18 cutting-edge models on three challenging benchmark datasets under four standard metrics. The code will be made publicly available.',
            imgs: [
              { src: 'Camouflaged Object.gif' }
            ],
            links: [
              { name: 'paper', link: 'https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Camouflaged_Object_Segmentation_With_Distraction_Mining_CVPR_2021_paper.pdf' },
              { name: '中译版', link: './CVPR2021_PFNet_CN.pdf' },
              { name: 'link', link: 'https://mhaiyang.github.io/CVPR2021_PFNet/index.html' },
            ]
          },
          {
            authors: 'Haiyang Mei, Xin Yang*, Yang Wang, Yuanyuan Liu, Shengfeng He, Qiang Zhang, Xiaopeng Wei, and Rynson Lau.',
            title: 'Don\'t Hit Me! Glass Detection in Real-world Scenes.',
            tags: [
              ['CVPR 2020', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input RGB image, our network outputs a binary mask that indicates where transparent glass regions are.',
            abstract: 'Transparent glass is very common in our daily life. Existing computer vision systems neglect it and thus may have severe consequences, e.g., a robot may crash into a glass wall. However, sensing the presence of glass is not straightforward. The key challenge is that arbitrary objects/scenes can appear behind the glass, and the content within the glass region is typically similar to those behind it. In this paper, we propose an important problem of detecting glass from a single RGB image. To address this problem, we construct a large-scale glass detection dataset (GDD) and design a glass detection network, called GDNet, which explores abundant contextual cues for robust glass detection with a novel large-field contextual feature integration (LCFI) module. Extensive experiments demonstrate that the proposed method achieves more superior glass detection results on our GDD test set than state-of-the-art methods fine-tuned for glass detection.',
            links: [
              { name: 'paper', link: 'http://www.cs.cityu.edu.hk/~rynson/papers/cvpr20d.pdf' },
              { name: 'suppl', link: 'http://www.cs.cityu.edu.hk/~rynson/papers/demos/cvpr20d-supp.pdf' },
              { name: 'dataset', link: '../data_application/DataApplication.html' },
              { name: 'media', link: 'https://www.sohu.com/a/408629893_610300?_trans_=000014_bdss_bddk' }
            ],
            imgs: [
              { src: '20200621135331.gif', label: 'Overview of proposed framework.' },
              { src: 'cvpr_2020_results.jpg', label: 'Problems with glass in existing vision tasks. In depth prediction, existing method [16] wrongly predicts the depth of the scene behind the glass, instead of the depth to the glass (1st row of (b)). For instance segmentation, Mask RCNN [9] only segments the instances behind the glass, not aware that they are actually behind the glass (2nd row of (b)). Besides, if we directly apply an existing singe-image reflection removal (SIRR) method [36] to an image that is only partially covered by glass, the non-glass region can be corrupted (3rd row of (b)). GDNet can detect the glass (c) and then correct these failure cases (d).' }
            ],
          },
          {
            authors: "Haiyang Mei, Yuanyuan Liu, Dongsheng Zhou, Xiaopeng Wei, Qiang Zhang, Xin Yang*.",
            title: "Exploring Dense Context for Salient Object Detection.",
            tags: [
              ["IEEE Transactions on Circuits and Systems for Video Technology 2021", 1],
              [". ", 0],
              ["(CCF B)", 2],
            ],
            io: 'Given an input RGB image, our network outputs a binary mask that indicates where salient objects are.',
            abstract:
              "Contexts play an important role in salient object detection (SOD). High-level contexts describe the relations between different parts/objects and thus are helpful for discovering the specific locations of salient objects while low-level contexts could provide the fine detail information for delineating the boundary of the salient objects. However, the way of perceiving/leveraging rich contexts has not been fully investigated by existing SOD works. The common context extraction strategies (e.g., leveraging convolutions with large kernels or atrous convolutions with large dilation rates) do not consider the effectiveness and efficiency simultaneously and may cause sub-optimal solutions. In this paper, we devote to exploring an effective and efficient way to learn rich contexts for accurate SOD. Specifically, we first build a dense context exploration (DCE) module to capture dense multi-scale contexts and further leverage the learned contexts to enhance the features discriminability. Then, we embed multiple DCE modules in an encoder-decoder architecture to harvest dense contexts of different levels. Furthermore, we propose an attentive skip-connection to transmit useful features from the encoder part to the decoder part for better dense context exploration. Finally, extensive experiments demonstrate that the proposed method achieves more superior detection results on the six benchmark datasets than 18 state-of-the-art SOD methods.",
            imgs: [
              { src: "Exploring Dense1.png" }, { src: "Exploring Dense2.png" }
            ],
            links: [
              { name: 'paper', link: 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9389751' },
              { name: 'link', link: 'https://mhaiyang.github.io/TCSVT2021-DCENet/index.html' },
            ]
          },
          {
            authors: "Xin Tian, Ke Xu, Xin Yang, Baocai Yin, Rynson Lau.",
            title: "Weakly-supervised Salient Instance Detection.",
            tags: [
              ["BMVC 2020", 1],
              [". ", 0],
              ["(Oral, 5%, Best Student Paper)", 2],
            ],
            io: 'Given an input RGB image, our network identifies each salient instance in the scene.',
            abstract:
              "Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. We note that subitizing information provides an instant judgement on the number of salient items, which naturally relates to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this insight, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is further fused to produce salient instance maps. We conduct extensive experiments to demonstrate that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks.",
            imgs: [
              { src: "Weakly-supervised Salient Instance Detection.png" },
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2009.13898' }
            ]
          },
          {
            authors:
              "Shimin Zhao, Miaomiao Chen, Pengjie Wang, Ying Cao, Pingping Zhang and Xin Yang.",
            title:
              "RGB-D Salient Object Detection via Deep Fusion of Semantics and Details.",
            tags: [
              ["Journal of Computer Animation and Virtual Worlds", 1],
              [", Bournemouth, UK. ", 0],
              ["(Special Issue of CASA 2020)", 2],
            ],
            imgs: [
              {
                src:
                  "RGB-D Salient Object Detection via Deep Fusion of Semantics and Details.png",
              },
            ],
            io: 'Given an input RGB-D image, our network outputs a binary mask that indicates where salient objects are.',
            abstract:
              "In this paper, we address RGB‐D salient object detection task by jointly leveraging semantics and contour details of salient objects. We propose a novel semantics‐and‐details complementary fusion network to adaptively integrate cross‐model and multilevel features. Specifically, we employ two kinds of fusion modules in our model, which are designed for fusing high‐level semantic features and integrating contour detail features of the scene components, respectively. The semantics fusion module aggregates high‐level interdependent semantic relationships by a nonlinear weighted summation of small and medium receptive fields. Meanwhile, the details module integrates multi‐level contour detail features to leverage expressive details of salient objects. We achieve new state‐of‐the‐art salient object detection results on seven RGB‐D datasets, that is, STERE, NJU2000, LFSD, NLPR, SSD, DES, and SIP2019 dataset. Experimental results demonstrate that our method outperforms eleven state‐of‐the‐art salient object detection methods. In this paper, we propose a novel semantics‐and‐details complementary fusion network to adaptively integrate cross‐model and multi‐level features. Specifically, we employ two kinds of fusion modules in our model, which are designed for fusing high‐level semantic features and integrating contour detail features of the scene components respectively. The semantics fusion module aggregates high‐level interdependent semantic relationships by a nonlinear weighted summation of small and medium receptive fields. Meanwhile, the details module integrates multi‐level contour detail features to leverage expressive details of salient objects.",
          },
          {
            authors:
              "Sucheng Ren, Chu Han, Xin Yang, Guoqiang Han, Shengfeng He.",
            title:
              "TENet: Triple Excitation Network for Video Salient Object Detection.",
            tags: [
              ["ECCV 2020", 1],
              [", Glasgow, UK. ", 0],
              ["(Spotlight)", 2],
            ],
            imgs: [
              {
                src:
                  "Triple Excitation Network for Video Salient Object Detection.png",
              },
            ],
            io: 'Given a series of frames {Tn|n = 1, 2, ..., N}, our network predicts the salient object in frame Tn.',
            abstract: 'In this paper, we propose a simple yet effective approach, named Triple Excitation Network, to reinforce the training of video salient object detection (VSOD) from three aspects, spatial, temporal, and online excitations. These excitation mechanisms are designed following the spirit of curriculum learning and aim to reduce learning ambiguities at the beginning of training by selectively exciting feature activations using ground truth. Then we gradually reduce the weight of ground truth excitations by a curriculum rate and replace it by a curriculum complementary map for better and faster convergence. In particular, the spatial excitation strengthens feature activations for clear object boundaries, while the temporal excitation imposes motions to emphasize spatio-temporal salient regions. Spatial and temporal excitations can combat the saliency shifting problem and conflict between spatial and temporal features of VSOD. Furthermore, our semi-curriculum learning design enables the first online refinement strategy for VSOD, which allows exciting and boosting saliency responses during testing without re-training. The proposed triple excitations can easily plug in different VSOD methods. Extensive experiments show the effectiveness of all three excitation methods and the proposed method outperforms state-of-the-art image and video salient object detection methods.'
          },
          {
            authors: 'Xin Yang*, Haiyang Mei*, Ke Xu, Xiaopeng Wei, Baocai Yin, and Rynson Lau (* joint first authors).',
            title: 'Where is My Mirror?',
            tags: [
              ['ICCV 2019', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input RGB image, our network outputs a binary mask that indicates where mirrors are.',
            abstract: 'Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial. The key challenge is that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this paper, we present a novel method to segment mirrors from an input image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions. First, we construct a large-scale mirror dataset that contains mirror images with corresponding manually annotated masks. This dataset covers a variety of daily life scenes, and will be made publicly available for future research. Second, we propose a novel network, called MirrorNet, for mirror segmentation, by modeling both semantical and low-level color/texture discontinuities between the contents inside and outside of the mirrors. Third, we conduct extensive experiments to evaluate the proposed method, and show that it outperforms the carefully chosen baselines from the state-of-the-art detection and segmentation methods.',
            links: [
              { name: 'paper', link: 'http://www.cs.cityu.edu.hk/~rynson/papers/iccv19a.pdf' },
              { name: 'suppl', link: 'http://www.cs.cityu.edu.hk/~rynson/papers/demos/iccv19a-supp.pdf' },
              { name: 'code and updated results', link: 'https://github.com/Mhaiyang/ICCV2019_MirrorNet' },
              { name: 'dataset', link: 'https://drive.google.com/file/d/1Znw92fO6lCKfXejjSSyMyL1qtFepgjPI/view?usp=sharing' },
              { name: 'media', link: 'https://www.qbitai.com/2019/09/7314.html' }
            ],
            imgs: [
              { src: '20200621135331(3).gif' },
              { src: 'iccv_2019_results.jpg', label: 'Problems with mirrors in existing vision tasks. In depth prediction, NYU-v2 dataset [32] uses a Kinect to capture depth as ground truth. It wrongly predicts the depths of the reflected contents, instead of the mirror depths (b). In instance semantic segmentation, Mask RCNN [12] wrongly detects objects inside the mirrors (c). With MirrorNet, we first detect and mask out the mirrors (d). We then obtain the correct depths (e), by interpolating the depths from surrounding pixels of the mirrors, and segmentation maps (f).' }
            ],
          }
        ],
      };
    },
    methods: {
      textBold(s) {
        return textBold(s);
      },
      goBack() {
        goBack();
      },
    },
  });
</script>

</html>