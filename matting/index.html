<!DOCTYPE html>
<html>
<title>Image Matting</title>

<head>
  <meta charset="UTF-8" />
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="../css/element-ui.css" />
  <link rel="stylesheet" href="../css/index.css" />
  <script src="../js/vue.min.js"></script>
  <script src="../js/element-ui.min.js"></script>
</head>

<body>
  <div id="app">
    <el-page-header @back="goBack" title="Back"> </el-page-header>
    <!-- Image Matting -->
    <div>
      <h1>Image Matting</h1>
      <el-divider></el-divider>
      <p>
        Matting refers to the process of extracting foreground
        object from an image. Matting is an important task in image
        and video editing. Matting tasks usually produces a "matte"
        that can be used to separate foreground from the background
        in a given image. Matte can also used to combine a given
        foreground on a different background to produce new
        plausible image. Image matting is a practical and heavily
        applied technique in image recognition, useful both on its
        own and as an intermediate stage in image and video
        processing.
      </p>
      <p>
        In this project, we are developing techniques to process
        Image Matting problems. Our research is to address this
        problem from two directions. The first is to consider how to
        solve the alpha matte form user-interactive manners. The
        second is to investigate how to extract accuract Alpha Matte
        in an end to end fashion while do not use any other auxilary
        information, such as trimap and scribble.
      </p>
      <el-divider></el-divider>
      <div v-for="project,index in projects">
        <p class="project_p">
          <span v-html="textBold(project.authors)"></span>
          <b class="project_title">{{project.title}}</b>
          <span v-for="tag in project.tags" :style="styles[tag[1]]">{{tag[0]}}</span>
          <span v-for="link in project.links">[<b><a class="project_links"
                :href="link.link">{{link.name}}</a></b><span>]&nbsp;</span></span>
        </p>
        <div v-if="project.video" class="figure_div">
          <video width="640" :src="'videos/'+project.video" controls></video>
        </div>
        <div v-for="img in project.imgs" class="figure_div">
          <img :src="'figures/'+img.src" :alt="img" class="figure_img" />
          <div v-if="img.label">
            <div class="figure_label">{{img.label}}</div>
            <br>
          </div>
        </div>
        <div v-if="project.io">
          <p><b>Input-Output:&nbsp;</b>{{project.io}}<span></span></p>
        </div>
        <div v-if="project.abstract">
          <p><b>Abstract.&nbsp;</b>{{project.abstract}}<span></span></p>
        </div>
        <el-divider content-position="right">{{index+1}}</el-divider>
      </div>
    </div>
  </div>
</body>
<script src="../js/index.js"></script>
<script>
  new Vue({
    el: '#app',
    data: function () {
      return {
        styles,
        projects: [
        {
            authors: 'Yu Qiao*, Yuhao Liu*, Ziqi Wei, Yuxin Wang, Qiang Cai, Guofeng Zhang, Xin Yang†.',
            title: 'Hierarchical and Progressive Image Matting.',
            abstract: 'Most matting researches resort to advanced semantics to achieve high-quality alpha mattes, and direct low-level features combination is usually explored to complement alpha details. However, we argue that appearance-agnostic integration can only provide biased foreground details and alpha mattes require differentlevel feature aggregation for better pixel-wise opacity perception. In this paper, we propose an end-to-end Hierarchical and Progressive Attention Matting Network (HAttMatting++), which can better predict the opacity of the foreground from single RGB images without additional input. Specifically, we utilize channel-wise attention to distill pyramidal features and employ spatial attention at different levels to filter appearance cues. This progressive attention mechanism can estimate alpha mattes from adaptive semantics and semanticsindicated boundaries. We also introduce a hybrid loss function fusing Structural SIMilarity (SSIM), Mean Square Error (MSE), Adversarial loss, and sentry supervision to guide the network to further improve the overall foreground structure. Besides, we construct a large-scale and challenging image matting dataset comprised of 59, 600 training images and 1000 test images (a total of 646 distinct foreground alpha mattes), which can further improve the robustness of our hierarchical and progressive aggregation model. Extensive experiments demonstrate that the proposed HAttMatting++ can capture sophisticated foreground structures and achieve state-of-the-art performance with single RGB images as input.',
            tags: [
              ['ACM Transactions on Multimedia Computing, Communications, and Applications(TOMM) 2022', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            imgs: [
              { src: 'Hierarchical-0.png' ,label:'Pipeline of our HAttMatting++. The orange box (Pyramidal Features Distillation) indicates channel-wise attention to distill pyramidal information extracted from ASPP [5]. The blue box (Appearance Cues Filtration) represents spatial attention to filter appearance cues, which are extracted from block1 and block2 in the feature extraction module.' },
              { src: 'Hierarchical-1.png' },
            ],
            links: [
              { name: 'paper', link: 'https://dl.acm.org/doi/abs/10.1145/3540201' }
            ],
          },
          {
            authors: 'Yu Qiao*, Ziqi Wei*, Yuhao Liu, Yuxin Wang†, Dongsheng Zhou, Qiang Zhang, Xin Yang.',
            title: 'Wider and Higher: Intensive Integration and Global Foreground Perception for Image Matting.',
            abstract: 'This paper reviews recent deep-learning-based matting research and conceives our wider and higher motivation for image matting. Many approaches achieve alpha mattes with complex encoders to extract robust semantics, then resort to the U-net-like decoder to concatenate or fuse encoder features. However, image matting is essentially a pixel-wise regression, and the ideal situation is to perceive the maximum opacity correspondence from the input image. In this paper, we argue that the high-resolution feature representation, perception and communication are more crucial for matting accuracy. Therefore, we propose an Intensive Integration and Global Foreground Perception network (I2GFP) to integrate wider and higher feature streams. Wider means we combine intensive features in each decoder stage, while higher suggests we retain high-resolution intermediate features and perceive large-scale foreground appearance. Our motivation sacrifices model depth for a significant performance promotion. We perform extensive experiments to prove the proposed I2GFP model, and state-of-the-art results can be achieved on different public datasets.',
            tags: [
              ['Computer Graphics International(CGI) 2022', 1],
              ['. ', 0]
            ],
            imgs: [
              { src: 'I2GFP-1.png', label: 'The overall architecture of the proposed Intensive Integration and Global Foreground Perception network (I2GFP). We employ simple backbone to extract necessary semantics and utilize intensive connections to integrate different-level features. The global foreground perception can capture rich appearances to complement details.' },
              { src: 'I2GFP-0.png' }
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2210.06919.pdf' }
            ]
          },
          {
            authors: 'Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, Xin Yang†.',
            title: 'Tripartite Information Mining and Integration for Image Matting.',
            io: 'Given an input RGB and the corresponding trimap, our method can produce a delicate alpha matte with the help of the integration of global and local information.',
            abstract: 'With the development of deep convolutional neural networks, image matting has ushered in a new phase. Regarding the nature of image matting, most researches have focused on solutions for transition regions. However, we argue that many existing approaches are excessively focused on transition-dominant local fields and ignored the inherent coordination between global information and transition optimisation. In this paper, we propose the Tripartite Information Mining and Integration Network (TIMI-Net) to harmonize the coordination between global and local attributes formally. Specifically, we resort to a novel 3-branch encoder to accomplish comprehensive mining of the input information, which can supplement the neglected coordination between global and local fields. In order to achieve effective and complete interaction between such multi-branches information, we develop the Tripartite Information Integration (TI^2) Module to transform and integrate the interconnections between the different branches. In addition, we built a large-scale human matting dataset (Human-2K) to advance human image matting, which consists of 2100 high-precision human images (2000 images for training and 100 images for test). Finally, we conduct extensive experiments to prove the performance of our proposed TIMI-Net, which demonstrates that our method performs favourably against the SOTA approaches on the alphamatting.com (Rank First), Composition-1K (MSE-0.006, Grad-11.5), Distinctions-646 and our Human-2K. Also, we have developed an online evaluation website to perform natural image matting.',
            tags: [
              ['ICCV 2021', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            imgs: [
              { src: 'TIMI-Net.gif' },
              { src: 'Tripartite-1.jpg', label: 'Visual comparisons of our method and Contex-Aware Matting [18] on the Real-World images. Both results are produced from the model trained on Adobe Composition-1K [55] dataset. Please zoom in to see the ﬁne details.' },
              { src: 'Tripartite-2.jpg', label: 'Almost all of the previous methods pay close attention to the local areas around the transition regions and may ignore the coordination between global and local information (texture and colour similarity, positional correlation, etc.). As shown in the above figure, the convolution kernel slides from left to right when performing a convolution operation. Types 1, 2 and 3 focus only on locally uncrossed ﬁelds (unknown or known regions). Only 4 and 5 perform enlightened transfers, with information ﬂowing from Bg and Fg to the transition region, respectively.' },
              { src: 'Tripartite-3.jpg', label: 'Based on the aforementioned observation, in this paper, we propose to supplement the superimposed branch with two auxiliary Units, named RGB-Unit and Trimap-Unit, for mining global appearance details and location cues, separately. In addition, we developed a Tripartite Information Integration module to integrate complementary information sufficiently.' }
            ],
            links: [
              { name: 'evaluation', link: 'http://47.111.168.199/TIMI-Net.html' }
            ],
          },
          {
            authors: 'Yuhao Liu∗, Jiake Xie∗, Yu Qiao, and Yong Tang, Xin Yang†.',
            title: 'Prior-Induced Information Alignment for Image Matting.',
            io: 'Given an input RGB and the corresponding trimap, our method can produces a delicate alpha matte with the help of  the dynamic gaussian modulation mechanism (DGM).',
            abstract: 'Image matting is an ill-posed problem that aims to estimate the opacity of foreground pixels in an image. However, most existing deep learning-based methods still suffer from the coarse-grained details. In general, these algorithms are incapable of felicitously distinguishing the degree of exploration between deterministic domains (e.g. certain FG and BG pixels) and undetermined domains (e.g. uncertain in-between pixels), or inevitably lose information in the continuous sampling process, leading to a sub-optimal result. In this paper, we propose a novel network named Prior-Induced Information Alignment Matting Network (PIIAMatting), which can efficiently model the distinction of pixel-wise response maps and the correlation of layer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation mechanism (DGM) and an Information Alignment strategy (IA). Specifically, the DGM can dynamically acquire a pixel-wise domain response map learned from the prior distribution. The response map can present the relationship between the opacity variation and the convergence process during training. On the other hand, the IA comprises an Information Match Module (IMM) and an Information Aggregation Module (IAM), jointly scheduled to match and aggregate the adjacent layer-wise features adaptively. Besides, we also develop a Multi- Scale Refinement (MSR) module to integrate multi-scale receptive field information at the refinement stage to recover the fluctu- ating appearance details. Extensive quantitative and qualitative evaluations demonstrate that the proposed PIIAMatting performs favourably against state-of-the-art image matting methods on the Alphamatting.com, Composition-1K and Distinctions-646 dataset.',
            tags: [
              ['IEEE Transactions on Multimedia(TMM) 2021', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            imgs: [
              { src: 'Prior-Induced-1.png' },
              { src: 'Prior-Induced-2.png' },
              { src: 'Prior-Induced-3.png' },
              { src: 'Prior-Induced-4.png' }
            ]
          },
          {
            authors: 'Yu Qiao, Yuhao Liu, Qiang Zhu, Xin Yang*, Yuxin Wang, Qiang Zhang，Xiaopeng Wei.',
            title: 'Multi-scale Information Assembly for Image Matting.',
            io: 'Given an input RGB image, our network directly produces an Alpha Matte without any auxilary information.',
            abstract: 'Image matting is a long-standing problem in computer graphics and vision, which mostly identified as the accurate estimation of the foreground in input images. We argue that the foreground objects can be represented by different-levels information, the main body, large-grained boundaries, refined details, etc. Based on this observation, in this paper, we propose a multi-scale information assembly framework (MSIA-matte) to pull out high-quality alpha mattes from single RGB images. Technically speaking, given an input image, we extract advanced semantics as our subject content, and retain initial CNNs features to encode different-levels foreground expressions, then combine them by our well-designed information assembly strategy. Extensive experiments have proven the effectiveness of the proposed MSIA-matte, and we can achieve state-of-the-art performance compared to existing matting networks.',
            tags: [
              ['Computer Graphics Forum (CGF)', 1],
              [' (Special Issue of Pacific Graphics 2020, Wellington, New Zealand). ', 0],
              ['(CCF B)', 2]],
            imgs: [
              { src: 'PG2020_Matting.gif' },
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2101.02391' },
            ],
          },
          {
            authors: 'Yu Qiao* , Yuhao Liu*, Xin Yang†, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, Xiaopeng Wei.',
            title:
              'Attention-Guided Hierarchical Structure Aggregation for Image Matting.',
            tags: [
              ['CVPR 2020', 1],
              ['. ', 0],
              ['(CCF A)', 2]],
            io: 'Given an input RGB image, our network directly produces an Alpha Matte without any auxilary information.',
            abstract:
              'Existing deep learning based matting algorithms primarily resort to high-level semantic features to improve the overall structure of alpha mattes. However, we argue that advanced semantics extracted from CNNs contribute unequally for alpha perception and we are supposed to reconcile advanced semantic information with low-level appearance cues to refine the foreground details. In this paper, we propose an end-to-end Hierarchical Attention Matting Network (HAttMatting), which can predict the better structure of alpha mattes from single RGB images without additional input. Specifically, we employ spatial and channel-wise attention to integrate appearance cues and pyramidal features in a novel fashion. This blended attention mechanism can perceive alpha mattes from refined boundaries and adaptive semantics. We also introduce a hybrid loss function fusing Structural SIMilarity (SSIM), Mean Square Error (MSE) and Adversarial loss to guide the network to further improve the overall foreground structure. Besides, we construct a large-scale image matting dataset comprised of 59,600 training images and $1000$ test images (total $646$ distinct foreground alpha mattes), which can further improve the robustness of our hierarchical structure aggregation model. Extensive experiments demonstrate that the proposed HAttMatting can capture sophisticated foreground structure and achieve state-of-the-art performance with single RGB images as input.',
            links: [
              { name: 'paper', link: 'http://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Attention-Guided_Hierarchical_Structure_Aggregation_for_Image_Matting_CVPR_2020_paper.pdf' },
              { name: 'suppl', link: 'http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Qiao_Attention-Guided_Hierarchical_Structure_CVPR_2020_supplemental.pdf' },
              { name: 'code', link: 'https://github.com/wukaoliu/CVPR2020-HAttMatting' },
              { name: 'dataset', link: '../data_application/DataApplication.html' },
              { name: 'media', link: 'https://www.sohu.com/a/403768845_651893?_f=index_pagefocus_7' }
            ],
            imgs: [
              { src: '20200621135332.gif' },
              { src: 'hattmatting_pipeline.png', label: 'Overview of proposed framework.' },
              { src: 'hattmatting_results.png', label: 'Visual comparison of our results with those of the state-of-the-art methods. Please zoom in to see the details.' },
            ],
          },
          {
            authors:
              'Xin Yang*, Yu Qiao*, Shaozhe Chen, Shengfeng He, Baocai Yin, Qiang Zhang, Xiaopeng Wei, Rynson W.H. Lau.',
            title:
              'Smart Scribbles for Image Matting.',
            tags: [
              ['ACM Transactions on Multimedia Computing, Communications, and Applications(TOMM) 2020', 1],
              ['. ', 0],
              ['(CCF B)', 2]
            ],
            io: 'Given an input RGB image, our network directly produces an Alpha Matte with only a few scribbles.',
            abstract: 'Image matting is an ill-posed problem that usually requires additional user input, such as trimaps or scribbles. Drawing a fine trimap requires a large amount of user effort, while using scribbles can hardly obtain satisfactory alpha mattes for non-professional users. Some recent deep learning based matting networks rely on large-scale composite datasets for training to improve performance, resulting in the occasional appearance of obvious artifacts when processing natural images. In this paper, we explore the intrinsic relationship between user input and alpha mattes, and strike a balance between user effort and the quality of alpha mattes. In particular, we propose an interactive framework, referred to as smart scribbles, to guide users to draw few scribbles on the input images to produce high-quality alpha mattes. It first infers the most informative regions of an image for drawing scribbles to indicate different categories (foreground, background or unknown), then spreads these scribbles (i.e., the category labels) to the rest of the image via our well-designed two-phase propagation. Both neighboring low-level affinities and high-level semantic features are considered during the propagation process. Our method can be optimized without large-scale matting datasets, and exhibits more universality in real situations. Extensive experiments demonstrate that smart scribbles can produce more accurate alpha mattes with reduced additional input, compared to the state-of-the-art matting methods.',
            imgs: [
              { src: 'TOMM_pipeline.png', label: 'Overview of proposed framework.' },
              { src: 'TOMM_results.png', label: 'Visual comparison of our results with those of the state-of-the-art methods. Please zoom in to see the details.' },
            ],
            links: [
              { name: 'paper', link: 'https://arxiv.org/pdf/2103.17062' },
            ],
          },
          {
            authors:
              'Xin Yang, Ke Xu, Shaozhe Chen, Shengfeng He, Baocai Yin, Rynson W.H. Lau.',
            title:
              'Active Matting.',
            tags: [
              ['NeurIPS 2018', 1],
              ['. ', 0],
              ['(CCF A)', 2]
            ],
            io: 'Given an input RGB image, our network directly produces an Alpha Matte with only a few points.',
            abstract: 'Image matting is an ill-posed problem. It requires a user input trimap or some strokes to obtain an alpha matte of the foreground object. A fine user input is essential to obtain a good result, which is either time consuming or suitable for experienced users who know where to place the strokes. In this paper, we explore the intrinsic relationship between the user input and the matting algorithm to address the problem of where and when the user should provide the input. Our aim is to discover the most informative sequence of regions for user input in order to produce a good alpha matte with minimum labeling efforts. To this end, we propose an active matting method with recurrent reinforcement learning. The proposed framework involves human in the loop by sequentially detecting informative regions for trivial human judgement. Comparing to traditional matting algorithms, the proposed framework requires much less efforts, and can produce satisfactory results with just 10 regions. Through extensive experiments, we show that the proposed model reduces user efforts significantly and achieves comparable performance to dense trimaps in a user-friendly manner. We further show that the learned informative knowledge can be generalized across different matting algorithms.',
            links: [
              { name: 'paper', link: 'http://papers.neurips.cc/paper/7710-active-matting.pdf' },
              { name: 'suppl', link: 'https://papers.nips.cc/paper/7710-active-matting-supplemental.zip' },
              { name: 'poster', link: 'http://www.cs.cityu.edu.hk/~rynson/papers/demos/nips18poster.pdf' },
              { name: 'dataset', link: 'http://www.cs.cityu.edu.hk/~rynson/papers/demos/rendered-100.zip' },
              { name: 'video', link: 'https://youtu.be/QcCBvS79fNE' }
            ],
            imgs: [
              { src: 'NIPS_pipeline.png', label: 'Overview of proposed framework.' },
              { src: 'NIPS_results.png', label: 'Visual comparison of our results with those of the state-of-the-art methods. Please zoom in to see the details.' },
            ],
          },
        ],
      }
    },
    methods: {
      textBold(s) {
        return textBold(s)
      },
      goBack() {
        goBack()
      },
    },
  })
</script>

</html>